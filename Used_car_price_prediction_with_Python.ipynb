{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Used car price prediction with Python.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting Used Car Price for Sales\n",
        "\n",
        "## 1. Data Exploration\n",
        "\n",
        "### 1.1. Reading the Data"
      ],
      "metadata": {
        "id": "5cILv43P6hmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"data/toyota.csv\")\n",
        "\n",
        "​X_1 = df.iloc[:,3:]\n",
        "\n",
        "X_2 = df.iloc[:,0:2]\n",
        "\n",
        "X = pd.concat([X_1, X_2], 1)\n",
        "\n",
        "y = df.iloc[:,2]\n",
        "\n",
        "print(X)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "j82Uv8Bt7CdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2. Observing the Variables"
      ],
      "metadata": {
        "id": "PEpq7GNY7EZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sorting with respect to \"price\" to check the min and max value\n",
        "\n",
        "df.sort_values(\"price\")\n",
        "\n",
        "print(df.describe()) # to analyze count, mean, std etc. of each variables\n",
        "print(df.info()) # to observe variable data types and to check on null values"
      ],
      "metadata": {
        "id": "d482DuiC7XKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains information about model, year, price, transmission, mileage, fuel type, road tax, mpg and engine size. The table above shows that the dataset contains no missing values so that changing null values are not needed at the preprocessing steps. Also, it can be seen that model, transmission and fuel type are categorical variables. This implies that one hot encoding operations should be performed to transform categorical variables into numerical ones.\n",
        "### 1.3. Data Visualization\n"
      ],
      "metadata": {
        "id": "W2V7qsiO7ZGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.pairplot(df, x_vars=[\"price\", \"year\", \"mileage\"], y_vars=[\"price\"], aspect=1, height=5)\n",
        "\n",
        "sns.pairplot(df, x_vars=[\"tax\", \"mpg\", \"engineSize\"], y_vars=[\"price\"], aspect=1, height=5)\n"
      ],
      "metadata": {
        "id": "0UlY2daT7nv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the graphs above, it can be seen that \"price\" has a negative correlation with \"year\" and positive correlation with \"mileage\". It can be said that both variables are realted with \"price\" and they have a relatively high coefficients. From the second graph, it is observed the relations between the independent variables. This graph is more like a proof of some preobserved hypothesis. For example, \"tax\" is increasing as \"engineSize\" increases.\n",
        "\n",
        "After this, \"price\" should be rescaled. It is a discrete, high range distributed value. To perform linear regression on this variable, the density graph should as much as similar to look like a normal distribution. That is why, some scaling operations such as logging, square-rooting or cube-rooting are performed. At the end, logging operation is selected since it gives the closest normal distribution of price.\n"
      ],
      "metadata": {
        "id": "1BYLc9f97pzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Observing the distribution of the \"price\" Data\n",
        "\n",
        "%matplotlib inline\n",
        "sns.displot(y)\n",
        "\n",
        "%matplotlib inline\n",
        "sns.displot(np.log10(y))\n",
        "\n",
        "%matplotlib inline\n",
        "sns.displot(np.sqrt(y))\n",
        "\n",
        "%matplotlib inline\n",
        "sns.displot(np.cbrt(y))\n"
      ],
      "metadata": {
        "id": "hwBa5R8s71_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "If logging is performed on price, it is the closest to normal distribution. So, price value should be changed to its logged version."
      ],
      "metadata": {
        "id": "rx7k_MQk76MR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.log10(y)"
      ],
      "metadata": {
        "id": "AJe87o3o79u7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Preprocessing\n",
        "\n",
        "Here, I performed one hot encoding. Cleaning the data from null values is ignored since the data doesn't involve any. It was observed from the previous .info() method on the data.\n",
        "### One Hot Encoding\n",
        "\n",
        "This step is required since some of the variables are categorical and contains string values which should be changed to numeraical values to efficiently perform the models. These variables are 'transmission', 'fuelType' and 'model'. One hot encoding creates dummy columns for each of elements of each of the variables.\n",
        "\n",
        "The result X data set of one hot encoding includes 31 columns while it has originally 8 columns."
      ],
      "metadata": {
        "id": "Z3w2BuiG7_bt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in [\"model\", \"fuelType\", \"transmission\"]:\n",
        "\n",
        "    unique = list(X[col].unique())\n",
        "    print(f\"{col} has {len(unique)} unique values:\" )\n",
        "    print(unique)"
      ],
      "metadata": {
        "id": "eWALPMxk8Mub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import make_column_transformer\n",
        "\n",
        "# One Hot Encoding for 'transmission' \n",
        "\n",
        "encoder = OneHotEncoder(sparse = False)\n",
        "x_encoded_1 = pd.DataFrame (encoder.fit_transform(X[['transmission']]))\n",
        "x_encoded_1.columns = encoder.get_feature_names(['transmission'])\n",
        "\n",
        "# One Hot Encoding for 'fuelType' \n",
        "x_encoded_2 = pd.DataFrame (encoder.fit_transform(X[['fuelType']]))\n",
        "x_encoded_2.columns = encoder.get_feature_names(['fuelType'])\n",
        "\n",
        "# One Hot Encoding for 'model' \n",
        "x_encoded_3 = pd.DataFrame (encoder.fit_transform(X[['model']]))\n",
        "x_encoded_3.columns = encoder.get_feature_names(['model'])\n",
        "\n",
        "x_encoded= pd.concat([x_encoded_1, x_encoded_2, x_encoded_3, X.iloc[:,3:6], X.iloc[:,-1], X.iloc[:,1]],1)\n",
        "\n",
        "​\n",
        "df = pd.concat([x_encoded, y],1)"
      ],
      "metadata": {
        "id": "TGTUomvQ8iOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Feature Selection\n",
        "### 3.1. Correlation Heat Map\n",
        "\n",
        "This is a step to check if the independent variables are correlated each other, it is crucial as including correlated independent variables decrease the efficieny of the model. Another aim of heat map is to check whether the dependent values are correlated with independent variable."
      ],
      "metadata": {
        "id": "i0ZeTYrt8je8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cor = df.astype(int).corr()\n",
        "plt.figure(figsize=(40, 20))\n",
        "\n",
        "sns.heatmap(cor, annot=True)\n",
        "\n",
        "columns = np.full((cor.shape[0],), True, dtype=bool)\n",
        "\n",
        "# Selecting of Variables Regarding Correlation Heat Map\n",
        "\n",
        "for i in range(cor.shape[0]):\n",
        "    for j in range(i+1, cor.shape[0]):\n",
        "        if cor.iloc[i, j] >= 0.75:\n",
        "            if columns[i]:\n",
        "                columns[j] = False\n",
        "\n",
        "print(df.columns[columns])\n",
        "selected_columns = df.columns[columns]\n",
        "\n",
        "#df = df[selected_columns]\n"
      ],
      "metadata": {
        "id": "lTFO5yt280An"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correlation heat map suggests that 'fuelType_hybrid' is highly correlated with 'transmission_Automatic'. The variable of hybrid fuel type can be disregarded from the model. Also, 'transmission_Other', 'model_Avensis' and 'mpg' are weakly correlated with y. That is why these independent variables can be ignored in the model too. However, I will decide the result of the independent variables set after performing other feature selection methods.\n",
        "\n",
        "### 3.2. Backward Feature Elimination\n",
        "\n",
        "This is a feature elimination step. It starts with full model and eliminate independent variables stepwise with respect to p values of the linear model."
      ],
      "metadata": {
        "id": "2w53HoiY84ba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.regression.linear_model as sm\n",
        "\n",
        "selected_columns = x_encoded.columns[0:].values\n",
        "\n",
        "def backward_elimination(x, y, sl, cols):\n",
        "    numVars = len(x[0])\n",
        "    for i in range(0, numVars):\n",
        "        regressor_ols = sm.OLS(y, x).fit()\n",
        "        maxVar = max(regressor_ols.pvalues).astype(float)\n",
        "        if maxVar > sl:\n",
        "            for j in range(0, numVars - i):\n",
        "                if regressor_ols.pvalues[j].astype(float) == maxVar:\n",
        "                    x = np.delete(x, j, 1)\n",
        "                    cols = np.delete(cols, j)\n",
        "\n",
        "    regressor_ols.summary()\n",
        "    return x, cols\n",
        "\n",
        "SL = 0.01\n",
        "\n",
        "data_modeled, selected_columns_2 = backward_elimination(x_encoded.values, y.values, SL, selected_columns)\n",
        "\n",
        "print(selected_columns_2)\n",
        "print(len(selected_columns_2))\n",
        "print(data_modeled)"
      ],
      "metadata": {
        "id": "0g7foVDc9C-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backward elimination feature selection with significance level is 0.01 suggests that data set can be diminished to the 28 variables which are\n",
        "\n",
        "'transmission_Automatic' 'transmission_Manual' 'transmission_Other' 'transmission_Semi-Auto' 'fuelType_Diesel' 'fuelType_Hybrid' 'fuelType_Other' 'fuelType_Petrol' 'model_ Auris' 'model_ Avensis' 'model_ Aygo' 'model_ C-HR' 'model_ Camry' 'model_ Corolla' 'model_ GT86' 'model_ Hilux' 'model_ IQ' 'model_ Land Cruiser' 'model_ PROACE VERSO' 'model_ Prius' 'model_ RAV4' 'model_ Supra' 'model_ Urban Cruiser' 'model_ Verso' 'model_ Verso-S' 'model_ Yaris' 'year' 'mileage'\n",
        "\n",
        "This result supports the results of the correlation heat map.\n",
        "### 3.3. Lasso Method\n",
        "\n",
        "Last feature selection method is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the resulting statistical model.\n"
      ],
      "metadata": {
        "id": "IuayOa_b9IGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectFromModel, SelectKBest\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "# data split for lasso\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_encoded, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# pipeline for scaling and lasso model\n",
        "pipeline = Pipeline([('scaler',StandardScaler()),('model',Lasso())])\n",
        "\n",
        "# applying pipeline to our data\n",
        "search = GridSearchCV(pipeline,\n",
        "                      {'model__alpha':np.arange(0.1,10,0.1)},\n",
        "                      cv = 5, scoring=\"neg_mean_squared_error\",verbose=3\n",
        "                      )\n",
        "search.fit(X_train,y_train)\n",
        "\n",
        "# scores\n",
        "\n",
        "search.best_params_\n",
        "coefficients = search.best_estimator_.named_steps['model'].coef_\n",
        "importance = np.abs(coefficients)\n",
        "\n",
        "# now let's see which of the features are high in importance which are not\n",
        "print(np.array(x_encoded.columns)[importance > 0])\n",
        "np.array(x_encoded.columns)[importance == 0]"
      ],
      "metadata": {
        "id": "d0sqSDw79X5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lasso model suggests that the 'engineSize' and 'year' are significant variables for price. It results with deleting other 29 independent variables.\n",
        "\n",
        "## 4. Data Fitting to the Models\n",
        "\n",
        "After analyzed the data set, I observed that the y variable which is 'price' in this case is an integerly valued and linearly distributed discrete variable. So, I have decided to perform linear regression with full model and OLS model with feature selected model.\n",
        "For linear regression, LinearRegression() model of scikit-learn helped me. Here, model included all the variables. Instead for OLS regression, I have employed statsmodels package and I specified the features as input of the model. The independent variables are chosen here as the combination of previosly mentioned feature selection methods which are backward elimination and lasso method.\n",
        "### 4.1. Linear Regression with Full Model"
      ],
      "metadata": {
        "id": "ZUqoATza9a-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear Regression model\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Instantiate a LinearRegression classifier with default parameter values\n",
        "regr = LinearRegression()\n",
        "\n",
        "# Fit regr to the train set\n",
        "regr.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "lbfeSrnp9mT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score, max_error\n",
        "import matplotlib.pyplot as pt\n",
        "\n",
        "# Use regr to predict instances from the test set and store it\n",
        "y_pred_LR = regr.predict(X_test)\n",
        "\n",
        "# Get the accuracy score of regr model \n",
        "print(\"Accuracy of linear regression classifier: \", regr.score(X_test, y_test))\n",
        "print(f\"Max error of predictions: {max_error(y_pred_LR, y_test)}\")\n",
        "\n",
        "\n",
        "# The coefficients\n",
        "print(\"Coefficients: \\n\", regr.coef_)\n",
        "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred_LR))\n",
        "print(\"Coefficient of determination: %.2f\" % r2_score(y_test, y_pred_LR))"
      ],
      "metadata": {
        "id": "Feo-Sqth9tmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear regression with full model results in the accuracy of 0.95, 0.96 R<sup>2</sup> value.\n",
        "### 4.2. OLS Model with Feature Selected Models\n",
        "\n",
        "In this model, I considered two models based on the different feature selection methods."
      ],
      "metadata": {
        "id": "hxwHGSBB9uhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.api as sm \n",
        "#Columns of Backward Feature Selection Method \n",
        "\n",
        "column_bfe = ['transmission_Automatic', 'transmission_Manual', 'transmission_Other', 'transmission_Semi-Auto', 'fuelType_Diesel', 'fuelType_Hybrid', 'fuelType_Other', 'fuelType_Petrol', 'model_ Auris', 'model_ Avensis', 'model_ Aygo', 'model_ C-HR', 'model_ Camry', 'model_ Corolla', 'model_ GT86', 'model_ Hilux', 'model_ IQ', 'model_ Land Cruiser', 'model_ PROACE VERSO', 'model_ Prius', 'model_ RAV4', 'model_ Supra', 'model_ Urban Cruiser', 'model_ Verso', 'model_ Verso-S', 'model_ Yaris', 'year', 'mileage']\n",
        "\n",
        "x_bfe_train = X_train[column_bfe]\n",
        "x_bfe_test = X_test[column_bfe]\n",
        "\n",
        "model_bfe = sm.OLS(y_train, x_bfe_train).fit() \n",
        "y_pred_bfe = model_bfe.predict(x_bfe_test) \n",
        "\n",
        "print(model_bfe.rsquared)  \n",
        "print(f\"Max error of predictions based on backward feature selection: {max_error(y_pred_bfe, y_test)}\")\n",
        "\n",
        "#Columns of Lasso Method \n",
        "\n",
        "column_lasso = ['engineSize', 'year']\n",
        "\n",
        "x_lasso_train = X_train[column_lasso]\n",
        "x_lasso_test = X_test[column_lasso]\n",
        "\n",
        "model_lasso = sm.OLS(y_train, x_lasso_train).fit() \n",
        "y_pred_lasso = model_lasso.predict(x_lasso_test) \n",
        "\n",
        "print(model_lasso.rsquared)  \n",
        "print(f\"Max error of predictions based on lasso selection: {max_error(y_pred_lasso, y_test)}\")"
      ],
      "metadata": {
        "id": "oxDC1RZO96-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XM7Tx24c6Qgd"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "print(f\"Mean error of predictions of LM: {mean_absolute_error(y_pred_LR, y_test)}\")\n",
        "print(f\"Mean error of predictions of backward FS: {mean_absolute_error(y_pred_bfe, y_test)}\")\n",
        "print(f\"Mean error of predictions of lasso: {mean_absolute_error(y_pred_lasso, y_test)}\")\n",
        "\n",
        "# Metric to check what percentage is not within the 1500 range\n",
        "perc_LR = len([abs(y_pred_LR[i]-v) for i, v in enumerate(y_test) if abs(y_pred_LR[i]-v) > np.log10(1500)])\n",
        "\n",
        "y_pred_bfe_arr = y_pred_bfe.array\n",
        "perc_bfe = len([abs(y_pred_bfe_arr[i]-v) for i, v in enumerate(y_test) if abs(y_pred_bfe_arr[i]-v) > np.log10(1500)])\n",
        "\n",
        "y_pred_lasso_arr = y_pred_lasso.array\n",
        "perc_lasso = len([abs(y_pred_lasso_arr[i]-v) for i, v in enumerate(y_test) if abs(y_pred_lasso_arr[i]-v) > np.log10(1500)])\n",
        "\n",
        "\n",
        "print(\"%.4f of the test values are not within the range of £1500 in linear regression test.\" % perc_LR)\n",
        "print(\"%.4f of the test values are not within the range of £1500 in backward fs OLS test.\" % perc_bfe)\n",
        "print(\"%.4f of the test values are not within the range of £1500 in lasso selected.\" % perc_lasso)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The comperatively results suggest that the data set with lasso based feature selected columns give the highest R<sup>2</sup> on OLS model. As a result, even if the linear regression with full model and backward feature selected OLS model give good results, for complexity reasons and R<sup>2</sup> reasons, I would only choose 'engine size' and 'year' variables to predict a price of a used car as lasso selction suggested.\n",
        "\n",
        "Since the test data set is not large enough, the deep learning methods were not applicable here. And since data not very large accuracy might be affected. For a large dataset we might have better results.\n"
      ],
      "metadata": {
        "id": "j5pfCGG1-Fcs"
      }
    }
  ]
}